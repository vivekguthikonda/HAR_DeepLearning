{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Activity Recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "This project is to build a model that predicts the human activities such as Walking, Walking_Upstairs, Walking_Downstairs, Sitting, Standing or Laying.\n",
    "\n",
    "This dataset is collected from 30 persons(referred as subjects in this dataset), performing different activities with a smartphone to their waists. The data is recorded with the help of sensors (accelerometer and Gyroscope) in that smartphone. This experiment was video recorded to label the data manually.\n",
    "\n",
    "## How data was recorded\n",
    "\n",
    "By using the sensors(Gyroscope and accelerometer) in a smartphone, they have captured '3-axial linear acceleration'(_tAcc-XYZ_) from accelerometer and '3-axial angular velocity' (_tGyro-XYZ_) from Gyroscope with several variations. \n",
    "\n",
    "> prefix 't' in those metrics denotes time.\n",
    "\n",
    "> suffix 'XYZ' represents 3-axial signals in X , Y, and Z directions.\n",
    "\n",
    "### Feature names\n",
    "\n",
    "1. These sensor signals are preprocessed by applying noise filters and then sampled in fixed-width windows(sliding windows) of 2.56 seconds each with 50% overlap. ie., each window has 128 readings. \n",
    "\n",
    "2. From Each window, a feature vector was obtianed by calculating variables from the time and frequency domain.\n",
    "> In our dataset, each datapoint represents a window with different readings \n",
    "3. The accelertion signal was saperated into Body and Gravity acceleration signals(___tBodyAcc-XYZ___ and ___tGravityAcc-XYZ___) using some low pass filter with corner frequecy of 0.3Hz.\n",
    "\n",
    "4. After that, the body linear acceleration and angular velocity were derived in time to obtian _jerk signals_ (___tBodyAccJerk-XYZ___ and ___tBodyGyroJerk-XYZ___). \n",
    "\n",
    "5. The magnitude of these 3-dimensional signals were calculated using the Euclidian norm. This magnitudes are represented as features with names like _tBodyAccMag_, _tGravityAccMag_, _tBodyAccJerkMag_, _tBodyGyroMag_ and _tBodyGyroJerkMag_.\n",
    "\n",
    "6. Finally, We've got frequency domain signals from some of the available signals by applying a FFT (Fast Fourier Transform). These signals obtained were labeled with ___prefix 'f'___ just like original signals with ___prefix 't'___. These signals are labeled as ___fBodyAcc-XYZ___, ___fBodyGyroMag___ etc.,.\n",
    "\n",
    "7. These are the signals that we got so far.\n",
    "\t+ tBodyAcc-XYZ\n",
    "\t+ tGravityAcc-XYZ\n",
    "\t+ tBodyAccJerk-XYZ\n",
    "\t+ tBodyGyro-XYZ\n",
    "\t+ tBodyGyroJerk-XYZ\n",
    "\t+ tBodyAccMag\n",
    "\t+ tGravityAccMag\n",
    "\t+ tBodyAccJerkMag\n",
    "\t+ tBodyGyroMag\n",
    "\t+ tBodyGyroJerkMag\n",
    "\t+ fBodyAcc-XYZ\n",
    "\t+ fBodyAccJerk-XYZ\n",
    "\t+ fBodyGyro-XYZ\n",
    "\t+ fBodyAccMag\n",
    "\t+ fBodyAccJerkMag\n",
    "\t+ fBodyGyroMag\n",
    "\t+ fBodyGyroJerkMag\n",
    "\n",
    "8. We can esitmate some set of variables from the above signals. ie., We will estimate the following properties on each and every signal that we recoreded so far.\n",
    "\n",
    "\t+ ___mean()___: Mean value\n",
    "\t+ ___std()___: Standard deviation\n",
    "\t+ ___mad()___: Median absolute deviation \n",
    "\t+ ___max()___: Largest value in array\n",
    "\t+ ___min()___: Smallest value in array\n",
    "\t+ ___sma()___: Signal magnitude area\n",
    "\t+ ___energy()___: Energy measure. Sum of the squares divided by the number of values. \n",
    "\t+ ___iqr()___: Interquartile range \n",
    "\t+ ___entropy()___: Signal entropy\n",
    "\t+ ___arCoeff()___: Autorregresion coefficients with Burg order equal to 4\n",
    "\t+ ___correlation()___: correlation coefficient between two signals\n",
    "\t+ ___maxInds()___: index of the frequency component with largest magnitude\n",
    "\t+ ___meanFreq()___: Weighted average of the frequency components to obtain a mean frequency\n",
    "\t+ ___skewness()___: skewness of the frequency domain signal \n",
    "\t+ ___kurtosis()___: kurtosis of the frequency domain signal \n",
    "\t+ ___bandsEnergy()___: Energy of a frequency interval within the 64 bins of the FFT of each window.\n",
    "\t+ ___angle()___: Angle between to vectors.\n",
    "\n",
    "9. We can obtain some other vectors by taking the average of signals in a single window sample. These are used on the angle() variable'\n",
    "`\n",
    "\t+ gravityMean\n",
    "\t+ tBodyAccMean\n",
    "\t+ tBodyAccJerkMean\n",
    "\t+ tBodyGyroMean\n",
    "\t+ tBodyGyroJerkMean\n",
    "\n",
    "\n",
    "###  Y_Labels(Encoded)\n",
    "+ In the dataset, Y_labels are represented as numbers from 1 to 6 as their identifiers.\n",
    "\n",
    "\t- WALKING as __1__\n",
    "\t- WALKING_UPSTAIRS as __2__\n",
    "\t- WALKING_DOWNSTAIRS as __3__\n",
    "\t- SITTING as __4__\n",
    "\t- STANDING as __5__\n",
    "\t- LAYING as __6__\n",
    "    \n",
    "## Train and test data were saperated\n",
    " - The readings from ___70%___ of the volunteers were taken as ___trianing data___ and remaining ___30%___ subjects recordings were taken for ___test data___\n",
    " \n",
    "## Data\n",
    "\n",
    "* All the data is present in 'UCI_HAR_dataset/' folder in present working directory.\n",
    "     - Feature names are present in 'UCI_HAR_dataset/features.txt'\n",
    "     - ___Train Data___\n",
    "         - 'UCI_HAR_dataset/train/X_train.txt'\n",
    "         - 'UCI_HAR_dataset/train/subject_train.txt'\n",
    "         - 'UCI_HAR_dataset/train/y_train.txt'\n",
    "     - ___Test Data___\n",
    "         - 'UCI_HAR_dataset/test/X_test.txt'\n",
    "         - 'UCI_HAR_dataset/test/subject_test.txt'\n",
    "         - 'UCI_HAR_dataset/test/y_test.txt'\n",
    "         \n",
    "\n",
    "## Data Size :\n",
    "> 27 MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick overview of the dataset :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Accelerometer and Gyroscope readings are taken from 30 volunteers(referred as subjects) while performing the following 6 Activities.\n",
    "\n",
    "    1. Walking     \n",
    "    2. WalkingUpstairs \n",
    "    3. WalkingDownstairs \n",
    "    4. Standing \n",
    "    5. Sitting \n",
    "    6. Lying.\n",
    "\n",
    "\n",
    "* Readings are divided into a window of 2.56 seconds with 50% overlapping. \n",
    "\n",
    "* Accelerometer readings are divided into gravity acceleration and body acceleration readings,\n",
    "  which has x,y and z components each.\n",
    "\n",
    "* Gyroscope readings are the measure of angular velocities which has x,y and z components.\n",
    "\n",
    "* Jerk signals are calculated for BodyAcceleration readings.\n",
    "\n",
    "* Fourier Transforms are made on the above time readings to obtain frequency readings.\n",
    "\n",
    "* Now, on all the base signal readings., mean, max, mad, sma, arcoefficient, engerybands,entropy etc., are calculated for each window.\n",
    "\n",
    "* We get a feature vector of 561 features and these features are given in the dataset.\n",
    "\n",
    "* Each window of readings is a datapoint of 561 features.\n",
    "\n",
    "## Problem Framework\n",
    "\n",
    "* 30 subjects(volunteers) data is randomly split to 70%(21) test and 30%(7) train data.\n",
    "* Each datapoint corresponds one of the 6 Activities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    " + Given a new datapoint we have to predict the Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activities are the class labels\n",
    "# It is a 6 class classification\n",
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DATADIR = 'UCI_HAR_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data signals\n",
    "# Signals are from Accelerometer and Gyroscope\n",
    "# The signals are in x,y,z directions\n",
    "# Sensor signals are filtered to have only body acceleration\n",
    "# excluding the acceleration due to gravity\n",
    "# Triaxial acceleration from the accelerometer is total acceleration\n",
    "SIGNALS = [\n",
    "    \"body_acc_x\",\n",
    "    \"body_acc_y\",\n",
    "    \"body_acc_z\",\n",
    "    \"body_gyro_x\",\n",
    "    \"body_gyro_y\",\n",
    "    \"body_gyro_z\",\n",
    "    \"total_acc_x\",\n",
    "    \"total_acc_y\",\n",
    "    \"total_acc_z\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to read the data from csv file\n",
    "def _read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "# Utility function to load the load\n",
    "def load_signals(subset):\n",
    "    signals_data = []\n",
    "\n",
    "    for signal in SIGNALS:\n",
    "        filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "        signals_data.append(\n",
    "            _read_csv(filename).as_matrix()\n",
    "        ) \n",
    "\n",
    "    # Transpose is used to change the dimensionality of the output,\n",
    "    # aggregating the signals by combination of sample/timestep.\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    return np.transpose(signals_data, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_y(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "\n",
    "    return pd.get_dummies(y).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test = load_signals('train'), load_signals('test')\n",
    "    y_train, y_test = load_y('train'), load_y('test')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing tensorflow\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring a session\n",
    "session_conf = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Keras\n",
    "from keras import backend as K\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, BatchNormalization, Flatten\n",
    "from keras.layers.core import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Loading the train and test data\n",
    "X_train, X_test, Y_train, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to count the number of classes\n",
    "def _count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "9\n",
      "7352\n"
     ]
    }
   ],
   "source": [
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = _count_classes(Y_train)\n",
    "\n",
    "print(timesteps)\n",
    "print(input_dim)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defining the Architecture of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "X_train  =  load_signals('train')\n",
    "Y_train  =  load_y('train')\n",
    "X_test  =  load_signals('test')\n",
    "Y_test  =  load_y('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_int_train = np.array([np.where(i == 1)[0][0]+1  for i in Y_train])\n",
    "y_int_test = np.array([np.where(i == 1)[0][0]+1  for i in Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train =  pd.DataFrame(y_int_train, columns = ['label'])\n",
    "x_train = pd.DataFrame(X_train.reshape(X_train.shape[0], 128*9))\n",
    "\n",
    "y_test =  pd.DataFrame(y_int_test, columns = ['label'])\n",
    "x_test = pd.DataFrame(X_test.reshape(X_test.shape[0] , 128*9))\n",
    "\n",
    "data_train = pd.DataFrame(pd.concat([x_train,y_train], axis =1))\n",
    "data_test = pd.DataFrame(pd.concat([x_test,y_test], axis =1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dynamic_train = data_train[data_train['label'].isin([1,2,3])]\n",
    "data_dynamic_test = data_test[data_test['label'].isin([1,2,3])]\n",
    "\n",
    "data_static_train = data_train[data_train['label'].isin([4,5,6])]\n",
    "data_static_test = data_test[data_test['label'].isin([4,5,6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "data_dynamic_train['label'] = 1\n",
    "data_dynamic_test['label'] = 1\n",
    "\n",
    "data_static_train['label'] = 0\n",
    "data_static_test['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "data_dynsta_train = shuffle(pd.concat([data_dynamic_train, data_static_train],axis =0))\n",
    "data_dynsta_test = shuffle(pd.concat([data_dynamic_test, data_static_test],axis =0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dynsta_train = np.array(data_dynsta_train.drop(['label'],axis=1)).reshape(data_dynsta_train.shape[0],128,9)\n",
    "Y_dynsta_train = data_dynsta_train['label']\n",
    "\n",
    "X_dynsta_test = np.array(data_dynsta_test.drop(['label'],axis=1)).reshape(data_dynsta_test.shape[0],128,9)\n",
    "Y_dynsta_test = data_dynsta_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "Y_dynsta_train = to_categorical(Y_dynsta_train)\n",
    "Y_dynsta_test = to_categorical(Y_dynsta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D,Add,Input, Activation,MaxPooling1D, Concatenate, MaxPooling2D ,Reshape, Conv2D, Flatten, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 64, 64)            2368      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 68,570\n",
      "Trainable params: 68,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Conv1D(64, input_shape = (128, 9), kernel_size = 4,  strides = 2, kernel_initializer = 'lecun_normal', padding = 'same'))\n",
    "model1.add(Dropout(0.7))\n",
    "\n",
    "model1.add(LSTM(100,  activation = 'softsign',recurrent_dropout = 0.2, kernel_initializer = 'lecun_normal',  dropout=0.5))\n",
    "model1.add(Dropout(0.85))\n",
    "\n",
    "model1.add(Dense(2, activation = 'softmax'))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.adam(lr = 0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 20)\n",
    "reducelr = ReduceLROnPlateau(monitor = 'val_loss',rate = 0.95,mode='min',verbose = 1,patience = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/50\n",
      "7352/7352 [==============================] - 3s 434us/step - loss: 0.6973 - acc: 0.5666 - val_loss: 0.5142 - val_acc: 0.7004\n",
      "Epoch 2/50\n",
      "7352/7352 [==============================] - 1s 107us/step - loss: 0.5386 - acc: 0.7013 - val_loss: 0.5412 - val_acc: 0.7730\n",
      "Epoch 3/50\n",
      "7352/7352 [==============================] - 1s 108us/step - loss: 0.4286 - acc: 0.7850 - val_loss: 1.0835 - val_acc: 0.8018\n",
      "Epoch 4/50\n",
      "7352/7352 [==============================] - 1s 107us/step - loss: 0.3546 - acc: 0.8543 - val_loss: 0.2787 - val_acc: 0.8890\n",
      "Epoch 5/50\n",
      "7352/7352 [==============================] - 1s 108us/step - loss: 0.2520 - acc: 0.9158 - val_loss: 0.5794 - val_acc: 0.7852\n",
      "Epoch 6/50\n",
      "7352/7352 [==============================] - 1s 107us/step - loss: 0.1651 - acc: 0.9550 - val_loss: 0.1797 - val_acc: 0.9498\n",
      "Epoch 7/50\n",
      "7352/7352 [==============================] - 1s 107us/step - loss: 0.1089 - acc: 0.9735 - val_loss: 0.1267 - val_acc: 0.9640\n",
      "Epoch 8/50\n",
      "7352/7352 [==============================] - 1s 107us/step - loss: 0.0764 - acc: 0.9825 - val_loss: 0.1326 - val_acc: 0.9739\n",
      "Epoch 9/50\n",
      "7352/7352 [==============================] - 1s 107us/step - loss: 0.0556 - acc: 0.9902 - val_loss: 0.1089 - val_acc: 0.9837\n",
      "Epoch 10/50\n",
      "7352/7352 [==============================] - 1s 107us/step - loss: 0.0483 - acc: 0.9914 - val_loss: 0.0477 - val_acc: 0.9912\n",
      "Epoch 11/50\n",
      "7352/7352 [==============================] - 1s 108us/step - loss: 0.0349 - acc: 0.9933 - val_loss: 0.0379 - val_acc: 0.9936\n",
      "Epoch 12/50\n",
      "7352/7352 [==============================] - 1s 131us/step - loss: 0.0347 - acc: 0.9948 - val_loss: 0.0311 - val_acc: 0.9959\n",
      "Epoch 13/50\n",
      "7352/7352 [==============================] - 1s 115us/step - loss: 0.0274 - acc: 0.9956 - val_loss: 0.0268 - val_acc: 0.9959\n",
      "Epoch 14/50\n",
      "7352/7352 [==============================] - 1s 112us/step - loss: 0.0187 - acc: 0.9973 - val_loss: 0.0205 - val_acc: 0.9969\n",
      "Epoch 15/50\n",
      "7352/7352 [==============================] - 1s 113us/step - loss: 0.0185 - acc: 0.9974 - val_loss: 0.0188 - val_acc: 0.9969\n",
      "Epoch 16/50\n",
      "7352/7352 [==============================] - 1s 109us/step - loss: 0.0131 - acc: 0.9981 - val_loss: 0.0193 - val_acc: 0.9973\n",
      "Epoch 17/50\n",
      "7352/7352 [==============================] - 1s 110us/step - loss: 0.0148 - acc: 0.9978 - val_loss: 0.0196 - val_acc: 0.9973\n",
      "Epoch 18/50\n",
      "7352/7352 [==============================] - 1s 110us/step - loss: 0.0150 - acc: 0.9980 - val_loss: 0.0262 - val_acc: 0.9969\n",
      "Epoch 19/50\n",
      "7352/7352 [==============================] - 1s 112us/step - loss: 0.0113 - acc: 0.9985 - val_loss: 0.0266 - val_acc: 0.9969\n",
      "Epoch 20/50\n",
      "7352/7352 [==============================] - 1s 110us/step - loss: 0.0125 - acc: 0.9982 - val_loss: 0.0219 - val_acc: 0.9973\n",
      "Epoch 21/50\n",
      "7352/7352 [==============================] - 1s 109us/step - loss: 0.0098 - acc: 0.9986 - val_loss: 0.0273 - val_acc: 0.9966\n",
      "Epoch 22/50\n",
      "7352/7352 [==============================] - 1s 110us/step - loss: 0.0148 - acc: 0.9977 - val_loss: 0.0240 - val_acc: 0.9969\n",
      "Epoch 23/50\n",
      "7352/7352 [==============================] - 1s 110us/step - loss: 0.0088 - acc: 0.9989 - val_loss: 0.0190 - val_acc: 0.9973\n",
      "Epoch 24/50\n",
      "7352/7352 [==============================] - 1s 109us/step - loss: 0.0102 - acc: 0.9985 - val_loss: 0.0353 - val_acc: 0.9956\n",
      "Epoch 25/50\n",
      "7352/7352 [==============================] - 1s 110us/step - loss: 0.0124 - acc: 0.9984 - val_loss: 0.0174 - val_acc: 0.9976\n",
      "Epoch 26/50\n",
      "7352/7352 [==============================] - 1s 110us/step - loss: 0.0037 - acc: 0.9993 - val_loss: 0.0162 - val_acc: 0.9976\n",
      "Epoch 27/50\n",
      "7352/7352 [==============================] - 1s 110us/step - loss: 0.0062 - acc: 0.9989 - val_loss: 0.0182 - val_acc: 0.9976\n",
      "Epoch 28/50\n",
      "7352/7352 [==============================] - 1s 115us/step - loss: 0.0067 - acc: 0.9992 - val_loss: 0.0163 - val_acc: 0.9976\n",
      "Epoch 29/50\n",
      "7352/7352 [==============================] - 1s 109us/step - loss: 0.0072 - acc: 0.9981 - val_loss: 0.0168 - val_acc: 0.9980\n",
      "Epoch 30/50\n",
      "7352/7352 [==============================] - 1s 114us/step - loss: 0.0127 - acc: 0.9980 - val_loss: 0.0174 - val_acc: 0.9980\n",
      "Epoch 31/50\n",
      "7352/7352 [==============================] - 1s 111us/step - loss: 0.0046 - acc: 0.9990 - val_loss: 0.0200 - val_acc: 0.9976\n",
      "Epoch 32/50\n",
      "7352/7352 [==============================] - 1s 111us/step - loss: 0.0070 - acc: 0.9985 - val_loss: 0.0218 - val_acc: 0.9976\n",
      "Epoch 33/50\n",
      "7352/7352 [==============================] - 1s 115us/step - loss: 0.0056 - acc: 0.9992 - val_loss: 0.0432 - val_acc: 0.9939\n",
      "Epoch 34/50\n",
      "7352/7352 [==============================] - 1s 112us/step - loss: 0.0049 - acc: 0.9993 - val_loss: 0.0746 - val_acc: 0.9905\n",
      "Epoch 35/50\n",
      "7352/7352 [==============================] - 1s 109us/step - loss: 0.0071 - acc: 0.9988 - val_loss: 0.0638 - val_acc: 0.9905\n",
      "Epoch 36/50\n",
      "7352/7352 [==============================] - 1s 112us/step - loss: 0.0106 - acc: 0.9980 - val_loss: 0.1061 - val_acc: 0.9871\n",
      "Epoch 37/50\n",
      "7352/7352 [==============================] - 1s 112us/step - loss: 0.0079 - acc: 0.9985 - val_loss: 0.1336 - val_acc: 0.9874\n",
      "Epoch 38/50\n",
      "7352/7352 [==============================] - 1s 111us/step - loss: 0.0060 - acc: 0.9990 - val_loss: 0.1324 - val_acc: 0.9874\n",
      "Epoch 39/50\n",
      "7352/7352 [==============================] - 1s 115us/step - loss: 0.0071 - acc: 0.9988 - val_loss: 0.1196 - val_acc: 0.9881\n",
      "Epoch 40/50\n",
      "7352/7352 [==============================] - 1s 112us/step - loss: 0.0086 - acc: 0.9990 - val_loss: 0.1045 - val_acc: 0.9885\n",
      "Epoch 41/50\n",
      "7352/7352 [==============================] - 1s 112us/step - loss: 0.0064 - acc: 0.9988 - val_loss: 0.0645 - val_acc: 0.9895\n",
      "Epoch 42/50\n",
      "7352/7352 [==============================] - 1s 111us/step - loss: 0.0052 - acc: 0.9992 - val_loss: 0.0510 - val_acc: 0.9932\n",
      "Epoch 43/50\n",
      "7352/7352 [==============================] - 1s 111us/step - loss: 0.0138 - acc: 0.9976 - val_loss: 0.0454 - val_acc: 0.9942\n",
      "Epoch 44/50\n",
      "7352/7352 [==============================] - 1s 116us/step - loss: 0.0121 - acc: 0.9986 - val_loss: 0.0124 - val_acc: 0.9976\n",
      "Epoch 45/50\n",
      "7352/7352 [==============================] - 1s 111us/step - loss: 0.0069 - acc: 0.9989 - val_loss: 0.0091 - val_acc: 0.9986\n",
      "Epoch 46/50\n",
      "7352/7352 [==============================] - 1s 112us/step - loss: 0.0041 - acc: 0.9996 - val_loss: 0.0105 - val_acc: 0.9983\n",
      "Epoch 47/50\n",
      "7352/7352 [==============================] - 1s 111us/step - loss: 0.0029 - acc: 0.9992 - val_loss: 0.0146 - val_acc: 0.9969\n",
      "Epoch 48/50\n",
      "7352/7352 [==============================] - 1s 113us/step - loss: 0.0060 - acc: 0.9988 - val_loss: 0.0151 - val_acc: 0.9973\n",
      "Epoch 49/50\n",
      "7352/7352 [==============================] - 1s 111us/step - loss: 0.0042 - acc: 0.9995 - val_loss: 0.0133 - val_acc: 0.9980\n",
      "Epoch 50/50\n",
      "7352/7352 [==============================] - 1s 112us/step - loss: 0.0065 - acc: 0.9985 - val_loss: 0.0108 - val_acc: 0.9983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26532b03668>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model1.fit(X_dynsta_train,\n",
    "          Y_dynsta_train,\n",
    "          batch_size=4096,\n",
    "          validation_data=(X_dynsta_test, Y_dynsta_test),\n",
    "          epochs=50,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model1 = load_model('model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submodel(Dynamic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dynamic_train = shuffle(data_train[data_train['label'].isin([1,2,3])])\n",
    "data_dynamic_test = shuffle(data_test[data_test['label'].isin([1,2,3])])\n",
    "\n",
    "data_static_train = shuffle(data_train[data_train['label'].isin([4,5,6])])\n",
    "data_static_test = shuffle(data_test[data_test['label'].isin([4,5,6])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_static_train['label'] = data_static_train['label'].replace([4,5,6],[0,1,2])\n",
    "data_static_test['label'] = data_static_test['label'].replace([4,5,6],[0,1,2])\n",
    "data_dynamic_train['label'] = data_dynamic_train['label'].replace([1,2,3],[0,1,2])\n",
    "data_dynamic_test['label'] = data_dynamic_test['label'].replace([1,2,3],[0,1,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dynamic_train = np.array(data_dynamic_train.drop(['label'], axis = 1)).reshape(data_dynamic_train.shape[0],128,9)\n",
    "x_dynamic_test = np.array(data_dynamic_test.drop(['label'], axis = 1)).reshape(data_dynamic_test.shape[0],128,9)\n",
    "x_static_train = np.array(data_static_train.drop(['label'], axis = 1)).reshape(data_static_train.shape[0],128,9)\n",
    "x_static_test = np.array(data_static_test.drop(['label'], axis = 1)).reshape(data_static_test.shape[0],128,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dynamic_train = to_categorical(data_dynamic_train['label'])\n",
    "y_dynamic_test = to_categorical(data_dynamic_test['label'])\n",
    "y_static_train = to_categorical(data_static_train['label'])\n",
    "y_static_test = to_categorical(data_static_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 64, 48)            1776      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 48)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               59600     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 61,679\n",
      "Trainable params: 61,679\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Conv1D(48, input_shape = (128, 9), kernel_size = 4,  strides = 2, kernel_initializer = 'lecun_normal', padding = 'same'))\n",
    "model2.add(Dropout(0.65))\n",
    "\n",
    "model2.add(LSTM(100,  activation = 'softsign',recurrent_dropout = 0.2, kernel_initializer = 'lecun_normal',  dropout=0.5))\n",
    "model2.add(Dropout(0.8))\n",
    "\n",
    "model2.add(Dense(3, activation = 'softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.adam(lr = 0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3285 samples, validate on 1387 samples\n",
      "Epoch 1/100\n",
      "3285/3285 [==============================] - 52s 16ms/step - loss: 1.1821 - acc: 0.3230 - val_loss: 1.1186 - val_acc: 0.3576\n",
      "Epoch 2/100\n",
      "3285/3285 [==============================] - 0s 128us/step - loss: 1.1348 - acc: 0.3601 - val_loss: 1.1125 - val_acc: 0.3576\n",
      "Epoch 3/100\n",
      "3285/3285 [==============================] - 0s 120us/step - loss: 1.1133 - acc: 0.3775 - val_loss: 1.0787 - val_acc: 0.3576\n",
      "Epoch 4/100\n",
      "3285/3285 [==============================] - 0s 127us/step - loss: 1.0972 - acc: 0.3921 - val_loss: 1.0490 - val_acc: 0.3886\n",
      "Epoch 5/100\n",
      "3285/3285 [==============================] - 0s 123us/step - loss: 1.0802 - acc: 0.4079 - val_loss: 1.0320 - val_acc: 0.4737\n",
      "Epoch 6/100\n",
      "3285/3285 [==============================] - 0s 117us/step - loss: 1.0730 - acc: 0.4152 - val_loss: 1.0254 - val_acc: 0.5321\n",
      "Epoch 7/100\n",
      "3285/3285 [==============================] - 0s 126us/step - loss: 1.0579 - acc: 0.4332 - val_loss: 1.0196 - val_acc: 0.5234\n",
      "Epoch 8/100\n",
      "3285/3285 [==============================] - 0s 123us/step - loss: 1.0309 - acc: 0.4667 - val_loss: 1.0027 - val_acc: 0.5443\n",
      "Epoch 9/100\n",
      "3285/3285 [==============================] - 0s 127us/step - loss: 1.0042 - acc: 0.4904 - val_loss: 0.9738 - val_acc: 0.5588\n",
      "Epoch 10/100\n",
      "3285/3285 [==============================] - 0s 123us/step - loss: 0.9641 - acc: 0.5333 - val_loss: 0.9682 - val_acc: 0.5530\n",
      "Epoch 11/100\n",
      "3285/3285 [==============================] - 0s 118us/step - loss: 0.9255 - acc: 0.5431 - val_loss: 1.0954 - val_acc: 0.5133\n",
      "Epoch 12/100\n",
      "3285/3285 [==============================] - 0s 119us/step - loss: 0.8781 - acc: 0.5836 - val_loss: 0.9392 - val_acc: 0.6099\n",
      "Epoch 13/100\n",
      "3285/3285 [==============================] - 0s 119us/step - loss: 0.8212 - acc: 0.6228 - val_loss: 0.9628 - val_acc: 0.6431\n",
      "Epoch 14/100\n",
      "3285/3285 [==============================] - 0s 115us/step - loss: 0.7515 - acc: 0.6612 - val_loss: 0.9639 - val_acc: 0.6561\n",
      "Epoch 15/100\n",
      "3285/3285 [==============================] - 0s 121us/step - loss: 0.7240 - acc: 0.6858 - val_loss: 0.7046 - val_acc: 0.7332\n",
      "Epoch 16/100\n",
      "3285/3285 [==============================] - 0s 121us/step - loss: 0.6866 - acc: 0.7142 - val_loss: 0.8074 - val_acc: 0.7188\n",
      "Epoch 17/100\n",
      "3285/3285 [==============================] - 0s 117us/step - loss: 0.6059 - acc: 0.7686 - val_loss: 0.8294 - val_acc: 0.7145\n",
      "Epoch 18/100\n",
      "3285/3285 [==============================] - 0s 123us/step - loss: 0.5764 - acc: 0.7869 - val_loss: 0.7388 - val_acc: 0.7354\n",
      "Epoch 19/100\n",
      "3285/3285 [==============================] - 0s 123us/step - loss: 0.5406 - acc: 0.7957 - val_loss: 0.8141 - val_acc: 0.7404\n",
      "Epoch 20/100\n",
      "3285/3285 [==============================] - 0s 131us/step - loss: 0.5280 - acc: 0.8091 - val_loss: 0.6689 - val_acc: 0.7621\n",
      "Epoch 21/100\n",
      "3285/3285 [==============================] - 0s 119us/step - loss: 0.4896 - acc: 0.8183 - val_loss: 0.5843 - val_acc: 0.7851\n",
      "Epoch 22/100\n",
      "3285/3285 [==============================] - 0s 126us/step - loss: 0.4754 - acc: 0.8134 - val_loss: 0.5789 - val_acc: 0.7945\n",
      "Epoch 23/100\n",
      "3285/3285 [==============================] - 0s 130us/step - loss: 0.4356 - acc: 0.8481 - val_loss: 0.6622 - val_acc: 0.7844\n",
      "Epoch 24/100\n",
      "3285/3285 [==============================] - 0s 120us/step - loss: 0.4214 - acc: 0.8511 - val_loss: 0.5453 - val_acc: 0.8176\n",
      "Epoch 25/100\n",
      "3285/3285 [==============================] - 0s 134us/step - loss: 0.3684 - acc: 0.8746 - val_loss: 0.4547 - val_acc: 0.8508\n",
      "Epoch 26/100\n",
      "3285/3285 [==============================] - 0s 136us/step - loss: 0.3517 - acc: 0.8828 - val_loss: 0.4622 - val_acc: 0.8587\n",
      "Epoch 27/100\n",
      "3285/3285 [==============================] - 0s 118us/step - loss: 0.3351 - acc: 0.9002 - val_loss: 0.4614 - val_acc: 0.8616\n",
      "Epoch 28/100\n",
      "3285/3285 [==============================] - 0s 123us/step - loss: 0.3038 - acc: 0.9078 - val_loss: 0.4089 - val_acc: 0.8825\n",
      "Epoch 29/100\n",
      "3285/3285 [==============================] - 0s 120us/step - loss: 0.2945 - acc: 0.9072 - val_loss: 0.3290 - val_acc: 0.8991\n",
      "Epoch 30/100\n",
      "3285/3285 [==============================] - 0s 117us/step - loss: 0.2766 - acc: 0.9117 - val_loss: 0.2636 - val_acc: 0.9250\n",
      "Epoch 31/100\n",
      "3285/3285 [==============================] - 0s 124us/step - loss: 0.2439 - acc: 0.9215 - val_loss: 0.2518 - val_acc: 0.9257\n",
      "Epoch 32/100\n",
      "3285/3285 [==============================] - 0s 119us/step - loss: 0.2605 - acc: 0.9257 - val_loss: 0.2572 - val_acc: 0.9279\n",
      "Epoch 33/100\n",
      "3285/3285 [==============================] - 0s 118us/step - loss: 0.2119 - acc: 0.9324 - val_loss: 0.2752 - val_acc: 0.9279\n",
      "Epoch 34/100\n",
      "3285/3285 [==============================] - 0s 117us/step - loss: 0.2028 - acc: 0.9409 - val_loss: 0.2955 - val_acc: 0.9257\n",
      "Epoch 35/100\n",
      "3285/3285 [==============================] - 0s 117us/step - loss: 0.1926 - acc: 0.9470 - val_loss: 0.3533 - val_acc: 0.9236\n",
      "Epoch 36/100\n",
      "3285/3285 [==============================] - 0s 118us/step - loss: 0.2209 - acc: 0.9379 - val_loss: 0.2870 - val_acc: 0.9315\n",
      "Epoch 37/100\n",
      "3285/3285 [==============================] - 0s 116us/step - loss: 0.2029 - acc: 0.9440 - val_loss: 0.2493 - val_acc: 0.9373\n",
      "Epoch 38/100\n",
      "3285/3285 [==============================] - 0s 117us/step - loss: 0.2080 - acc: 0.9373 - val_loss: 0.1712 - val_acc: 0.9517\n",
      "Epoch 39/100\n",
      "3285/3285 [==============================] - 0s 117us/step - loss: 0.1705 - acc: 0.9470 - val_loss: 0.1597 - val_acc: 0.9539\n",
      "Epoch 40/100\n",
      "3285/3285 [==============================] - 0s 117us/step - loss: 0.1791 - acc: 0.9425 - val_loss: 0.1687 - val_acc: 0.9582\n",
      "Epoch 41/100\n",
      "3285/3285 [==============================] - 0s 116us/step - loss: 0.1771 - acc: 0.9412 - val_loss: 0.1668 - val_acc: 0.9553\n",
      "Epoch 42/100\n",
      "3285/3285 [==============================] - 0s 118us/step - loss: 0.1392 - acc: 0.9549 - val_loss: 0.1863 - val_acc: 0.9531\n",
      "Epoch 43/100\n",
      "3285/3285 [==============================] - 0s 117us/step - loss: 0.1582 - acc: 0.9546 - val_loss: 0.1954 - val_acc: 0.9495\n",
      "Epoch 44/100\n",
      "3285/3285 [==============================] - 0s 116us/step - loss: 0.1493 - acc: 0.9553 - val_loss: 0.2151 - val_acc: 0.9517\n",
      "Epoch 45/100\n",
      "3285/3285 [==============================] - 0s 125us/step - loss: 0.1354 - acc: 0.9632 - val_loss: 0.2396 - val_acc: 0.9503\n",
      "Epoch 46/100\n",
      "3285/3285 [==============================] - 0s 120us/step - loss: 0.1700 - acc: 0.9537 - val_loss: 0.1801 - val_acc: 0.9582\n",
      "Epoch 47/100\n",
      "3285/3285 [==============================] - 0s 119us/step - loss: 0.1168 - acc: 0.9647 - val_loss: 0.1334 - val_acc: 0.9625\n",
      "Epoch 48/100\n",
      "3285/3285 [==============================] - 0s 124us/step - loss: 0.1180 - acc: 0.9659 - val_loss: 0.1322 - val_acc: 0.9603\n",
      "Epoch 49/100\n",
      "3285/3285 [==============================] - 0s 118us/step - loss: 0.1142 - acc: 0.9632 - val_loss: 0.1084 - val_acc: 0.9654\n",
      "Epoch 50/100\n",
      "3285/3285 [==============================] - 0s 125us/step - loss: 0.1058 - acc: 0.9717 - val_loss: 0.1082 - val_acc: 0.9676\n",
      "Epoch 51/100\n",
      "3285/3285 [==============================] - 0s 124us/step - loss: 0.1223 - acc: 0.9629 - val_loss: 0.1117 - val_acc: 0.9661\n",
      "Epoch 52/100\n",
      "3285/3285 [==============================] - 0s 122us/step - loss: 0.1095 - acc: 0.9683 - val_loss: 0.1046 - val_acc: 0.9683\n",
      "Epoch 53/100\n",
      "3285/3285 [==============================] - 0s 118us/step - loss: 0.1093 - acc: 0.9680 - val_loss: 0.1119 - val_acc: 0.9697\n",
      "Epoch 54/100\n",
      "3285/3285 [==============================] - 0s 121us/step - loss: 0.1118 - acc: 0.9717 - val_loss: 0.1327 - val_acc: 0.9683\n",
      "Epoch 55/100\n",
      "3285/3285 [==============================] - 0s 127us/step - loss: 0.1214 - acc: 0.9653 - val_loss: 0.1263 - val_acc: 0.9683\n",
      "Epoch 56/100\n",
      "3285/3285 [==============================] - 0s 123us/step - loss: 0.0973 - acc: 0.9699 - val_loss: 0.1603 - val_acc: 0.9575\n",
      "Epoch 57/100\n",
      "3285/3285 [==============================] - 0s 118us/step - loss: 0.1094 - acc: 0.9680 - val_loss: 0.1857 - val_acc: 0.9553\n",
      "Epoch 58/100\n",
      "3285/3285 [==============================] - 0s 118us/step - loss: 0.1060 - acc: 0.9674 - val_loss: 0.1632 - val_acc: 0.9567\n",
      "Epoch 59/100\n",
      "3285/3285 [==============================] - 0s 133us/step - loss: 0.0811 - acc: 0.9772 - val_loss: 0.1269 - val_acc: 0.9603\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - 0s 123us/step - loss: 0.0839 - acc: 0.9781 - val_loss: 0.1135 - val_acc: 0.9625\n",
      "Epoch 61/100\n",
      "3285/3285 [==============================] - 0s 121us/step - loss: 0.0948 - acc: 0.9732 - val_loss: 0.1150 - val_acc: 0.9618\n",
      "Epoch 62/100\n",
      "3285/3285 [==============================] - 0s 125us/step - loss: 0.0801 - acc: 0.9753 - val_loss: 0.1333 - val_acc: 0.9603\n",
      "Epoch 63/100\n",
      "3285/3285 [==============================] - 0s 121us/step - loss: 0.0847 - acc: 0.9738 - val_loss: 0.1324 - val_acc: 0.9603\n",
      "Epoch 64/100\n",
      "3285/3285 [==============================] - 0s 123us/step - loss: 0.0778 - acc: 0.9760 - val_loss: 0.1275 - val_acc: 0.9589\n",
      "Epoch 65/100\n",
      "3285/3285 [==============================] - 0s 118us/step - loss: 0.0848 - acc: 0.9747 - val_loss: 0.1161 - val_acc: 0.9632\n",
      "Epoch 66/100\n",
      "3285/3285 [==============================] - 0s 121us/step - loss: 0.0694 - acc: 0.9784 - val_loss: 0.0912 - val_acc: 0.9690\n",
      "Epoch 67/100\n",
      "3285/3285 [==============================] - 0s 123us/step - loss: 0.0716 - acc: 0.9775 - val_loss: 0.0949 - val_acc: 0.9748\n",
      "Epoch 68/100\n",
      "3285/3285 [==============================] - 0s 125us/step - loss: 0.0673 - acc: 0.9775 - val_loss: 0.0921 - val_acc: 0.9755\n",
      "Epoch 69/100\n",
      "3285/3285 [==============================] - 0s 122us/step - loss: 0.0861 - acc: 0.9753 - val_loss: 0.0955 - val_acc: 0.9726\n",
      "Epoch 70/100\n",
      "3285/3285 [==============================] - 0s 122us/step - loss: 0.0642 - acc: 0.9799 - val_loss: 0.1362 - val_acc: 0.9676\n",
      "Epoch 71/100\n",
      "3285/3285 [==============================] - 0s 117us/step - loss: 0.0786 - acc: 0.9787 - val_loss: 0.1610 - val_acc: 0.9640\n",
      "Epoch 72/100\n",
      "3285/3285 [==============================] - 0s 125us/step - loss: 0.0973 - acc: 0.9732 - val_loss: 0.0995 - val_acc: 0.9762\n",
      "Epoch 73/100\n",
      "3285/3285 [==============================] - 0s 133us/step - loss: 0.0789 - acc: 0.9781 - val_loss: 0.0911 - val_acc: 0.9755\n",
      "Epoch 74/100\n",
      "3285/3285 [==============================] - 0s 124us/step - loss: 0.0756 - acc: 0.9744 - val_loss: 0.0813 - val_acc: 0.9712\n",
      "Epoch 75/100\n",
      "3285/3285 [==============================] - 0s 118us/step - loss: 0.0667 - acc: 0.9811 - val_loss: 0.1164 - val_acc: 0.9596\n",
      "Epoch 76/100\n",
      "3285/3285 [==============================] - 0s 129us/step - loss: 0.0702 - acc: 0.9778 - val_loss: 0.1497 - val_acc: 0.9582\n",
      "Epoch 77/100\n",
      "3285/3285 [==============================] - 0s 123us/step - loss: 0.0681 - acc: 0.9784 - val_loss: 0.1533 - val_acc: 0.9589\n",
      "Epoch 78/100\n",
      "3285/3285 [==============================] - 0s 121us/step - loss: 0.0628 - acc: 0.9833 - val_loss: 0.1472 - val_acc: 0.9654\n",
      "Epoch 79/100\n",
      "3285/3285 [==============================] - 0s 117us/step - loss: 0.0498 - acc: 0.9860 - val_loss: 0.1432 - val_acc: 0.9668\n",
      "Epoch 80/100\n",
      "3285/3285 [==============================] - 0s 127us/step - loss: 0.0681 - acc: 0.9790 - val_loss: 0.1259 - val_acc: 0.9683\n",
      "Epoch 81/100\n",
      "3285/3285 [==============================] - 0s 117us/step - loss: 0.0496 - acc: 0.9836 - val_loss: 0.1090 - val_acc: 0.9740\n",
      "Epoch 82/100\n",
      "3285/3285 [==============================] - 0s 128us/step - loss: 0.0512 - acc: 0.9845 - val_loss: 0.1035 - val_acc: 0.9776\n",
      "Epoch 83/100\n",
      "3285/3285 [==============================] - 0s 123us/step - loss: 0.0527 - acc: 0.9869 - val_loss: 0.1123 - val_acc: 0.9784\n",
      "Epoch 84/100\n",
      "3285/3285 [==============================] - 0s 119us/step - loss: 0.0644 - acc: 0.9848 - val_loss: 0.1167 - val_acc: 0.9798\n",
      "Epoch 85/100\n",
      "3285/3285 [==============================] - 0s 118us/step - loss: 0.0490 - acc: 0.9857 - val_loss: 0.1096 - val_acc: 0.9791\n",
      "Epoch 86/100\n",
      "3285/3285 [==============================] - 0s 127us/step - loss: 0.0527 - acc: 0.9830 - val_loss: 0.0972 - val_acc: 0.9805\n",
      "Epoch 87/100\n",
      "3285/3285 [==============================] - 0s 118us/step - loss: 0.0479 - acc: 0.9836 - val_loss: 0.1013 - val_acc: 0.9798\n",
      "Epoch 88/100\n",
      "3285/3285 [==============================] - 0s 119us/step - loss: 0.0522 - acc: 0.9848 - val_loss: 0.0906 - val_acc: 0.9805\n",
      "Epoch 89/100\n",
      "3285/3285 [==============================] - 0s 119us/step - loss: 0.0402 - acc: 0.9900 - val_loss: 0.0939 - val_acc: 0.9805\n",
      "Epoch 90/100\n",
      "3285/3285 [==============================] - 0s 122us/step - loss: 0.0456 - acc: 0.9875 - val_loss: 0.0996 - val_acc: 0.9791\n",
      "Epoch 91/100\n",
      "3285/3285 [==============================] - 0s 127us/step - loss: 0.0404 - acc: 0.9875 - val_loss: 0.1067 - val_acc: 0.9784\n",
      "Epoch 92/100\n",
      "3285/3285 [==============================] - 0s 119us/step - loss: 0.0422 - acc: 0.9900 - val_loss: 0.1328 - val_acc: 0.9748\n",
      "Epoch 93/100\n",
      "3285/3285 [==============================] - 0s 129us/step - loss: 0.0404 - acc: 0.9903 - val_loss: 0.1655 - val_acc: 0.9719\n",
      "Epoch 94/100\n",
      "3285/3285 [==============================] - 0s 126us/step - loss: 0.0446 - acc: 0.9866 - val_loss: 0.1628 - val_acc: 0.9719\n",
      "Epoch 95/100\n",
      "3285/3285 [==============================] - 0s 129us/step - loss: 0.0396 - acc: 0.9896 - val_loss: 0.1444 - val_acc: 0.9755\n",
      "Epoch 96/100\n",
      "3285/3285 [==============================] - 0s 119us/step - loss: 0.0376 - acc: 0.9866 - val_loss: 0.1104 - val_acc: 0.9805\n",
      "Epoch 97/100\n",
      "3285/3285 [==============================] - 0s 127us/step - loss: 0.0276 - acc: 0.9918 - val_loss: 0.1018 - val_acc: 0.9820\n",
      "Epoch 98/100\n",
      "3285/3285 [==============================] - 0s 131us/step - loss: 0.0425 - acc: 0.9893 - val_loss: 0.1109 - val_acc: 0.9798\n",
      "Epoch 99/100\n",
      "3285/3285 [==============================] - 0s 120us/step - loss: 0.0323 - acc: 0.9884 - val_loss: 0.0989 - val_acc: 0.9769\n",
      "Epoch 100/100\n",
      "3285/3285 [==============================] - 0s 119us/step - loss: 0.0345 - acc: 0.9884 - val_loss: 0.0714 - val_acc: 0.9856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c43cb611d0>"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model2.fit(x_dynamic_train,\n",
    "          y_dynamic_train,\n",
    "          batch_size=4096,\n",
    "          validation_data=(x_dynamic_test, y_dynamic_test),\n",
    "          epochs=100,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model2 = load_model('model2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submodel( Static )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_166 (Conv1D)          (None, 64, 48)            4800      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_142 (MaxPoolin (None, 12, 48)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 11, 64)            6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_143 (MaxPoolin (None, 5, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 4, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_144 (MaxPoolin (None, 2, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 2, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 19,651\n",
      "Trainable params: 19,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Conv1D(48, input_shape = (128, 9), kernel_size = 11,  strides = 2, kernel_initializer = 'lecun_normal', padding = 'same'))\n",
    "model3.add(MaxPooling1D(5))\n",
    "model3.add(Conv1D(64, kernel_size = 2,  strides = 1, kernel_initializer = 'lecun_normal', activation='relu'))\n",
    "model3.add(MaxPooling1D(2))\n",
    "model3.add(Conv1D(64, kernel_size = 2,  strides = 1, kernel_initializer = 'lecun_normal', activation='relu'))\n",
    "model3.add(MaxPooling1D(2))\n",
    "model3.add(Dropout(0.975))\n",
    "\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(3, activation = 'softmax'))\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.adam(lr = 0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26095d1cba8>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model3.fit(x_static_train,\n",
    "          y_static_train,\n",
    "          batch_size=4096,\n",
    "          validation_data=(x_static_test, y_static_test),\n",
    "          epochs=2000,\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "4067/4067 [==============================] - 0s 13us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09474246203899384, 0.9798377156257629]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(x_static_train, y_static_train, batch_size =4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1560/1560 [==============================] - 0s 13us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14470736682415009, 0.9557692408561707]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(x_static_test, y_static_test, batch_size =4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model3 = load_model('model3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final prediction using above 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def predict(x):\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    dynOrStat = np.argmax(model1.predict(x), axis = 1)\n",
    "    \n",
    "    x_dyn = x[dynOrStat == 1]\n",
    "    x_sta = x[dynOrStat == 0]\n",
    "    \n",
    "    predict_dynamic = np.argmax(model2.predict(x_dyn), axis = 1)\n",
    "    predict_static = np.argmax(model3.predict(x_sta), axis = 1)+3 \n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    for output in dynOrStat:\n",
    "        if output == 1:\n",
    "            predictions.append(predict_dynamic[i])\n",
    "            i = i + 1\n",
    "        else:\n",
    "            predictions.append(predict_static[j])\n",
    "            j = j + 1\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in Y_true])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in Y_pred])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                LAYING  SITTING  STANDING  WALKING  WALKING_DOWNSTAIRS  \\\n",
      "True                                                                         \n",
      "LAYING                 536        0         0        1                   0   \n",
      "SITTING                  3      441        46        0                   0   \n",
      "STANDING                 0       20       511        0                   0   \n",
      "WALKING                  0        0         1      483                   0   \n",
      "WALKING_DOWNSTAIRS       0        0         0        1                 416   \n",
      "WALKING_UPSTAIRS         0        0         1        2                   2   \n",
      "\n",
      "Pred                WALKING_UPSTAIRS  \n",
      "True                                  \n",
      "LAYING                             0  \n",
      "SITTING                            1  \n",
      "STANDING                           1  \n",
      "WALKING                           12  \n",
      "WALKING_DOWNSTAIRS                 3  \n",
      "WALKING_UPSTAIRS                 466  \n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(np.argmax(Y_test, axis =1 ), predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9880304678998912\n",
      "Test accuracy:  0.9681031557516118\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Train accuracy:',accuracy_score(np.argmax(Y_train, axis =1),predict(X_train)))\n",
    "print('Test accuracy: ',accuracy_score(np.argmax(Y_test, axis = 1),predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By using two seperate models for static and dynamic activities, we got Test accuracy of 96.8%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> End of Self Case Study  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
